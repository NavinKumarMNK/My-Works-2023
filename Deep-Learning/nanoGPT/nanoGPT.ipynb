{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d22db2c6",
   "metadata": {},
   "source": [
    "# **Generative PreTrained Transformer**\n",
    "\n",
    "- GPT is a language model that uses deep learning to generate human-like text. It is skilled at generating text that looks like it was written by a human. It is able to link ideas logically, defend them, adapt to the context, roleplay, and avoid contradicting itse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a134c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: NavinKumarMNK\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.10\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "torch     : 2.1.0a0+fe05266\n",
      "lightning : 2.0.2\n",
      "cudf      : 23.2.0\n",
      "sklearn   : 1.2.0\n",
      "polars    : 0.17.11\n",
      "pandas    : 1.5.2\n",
      "numpy     : 1.22.2\n",
      "matplotlib: 3.6.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'NavinKumarMNK' -v -p torch,lightning,cudf,sklearn,polars,pandas,numpy,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de34ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ffd261",
   "metadata": {},
   "source": [
    "## **Data Pre-Processing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc6e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-29 12:09:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.06M  2.10MB/s    in 0.5s    \n",
      "\n",
      "2023-05-29 12:09:26 (2.10 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314b24de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.txt  input.txt.1\tinput.txt.2  input.txt.3  nanoGPT.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c167040",
   "metadata": {},
   "source": [
    "### **Token Extraction** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71297194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 262927\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing in the form of words and character extracted from the tiny words corpus\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "\n",
    "print(tokens[0], len(tokens))\n",
    "\n",
    "chars = []\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens[i])):\n",
    "        if tokens[i][j] in chars:\n",
    "            pass\n",
    "        else:\n",
    "            chars.append(tokens[i][j])\n",
    "        \n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc07986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters :\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "if \"\\n\" not in chars:\n",
    "    chars.append(\"\\n\")\n",
    "if \" \" not in chars:\n",
    "    chars.append(\" \")\n",
    "\n",
    "chars = sorted(chars)\n",
    "print(\"Characters :\\n\", chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24f5ddef",
   "metadata": {},
   "source": [
    "## **Tokenization**\n",
    "- Google uses Sentence Piece : https://github.com/google/sentencepiece : text2int, sub-word units\n",
    "- OpenAI uses tiktoken : https://github.com/openai/tiktoken : BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb170ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi = {ch:i for i, ch in enumerate(chars)}\n",
    "iotc = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode_chars = lambda string: [ctoi[chars] for chars in string]\n",
    "decode_chars = lambda lst: [iotc[i] for i in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e2bd2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 53, 1, 51, 53, 56, 43, 1, 58, 39, 50, 49, 47, 52, 45, 1, 53, 52, 5, 58, 11, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 1, 42, 53, 52, 43, 10, 1, 39, 61, 39, 63, 6, 1, 39, 61, 39, 63, 2]\n"
     ]
    }
   ],
   "source": [
    "string = \"No more talking on't; let it be done: away, away!\"\n",
    "\n",
    "print(encode_chars(string))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3078101",
   "metadata": {},
   "source": [
    "## **Pre Requisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4ac5189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1947\n"
     ]
    }
   ],
   "source": [
    "BLOCK_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_SIZE = 32 \n",
    "\n",
    "pl.seed_everything(1947)\n",
    "np.random.seed(1947)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92b24d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the fearful king,\n",
      "And this th\n"
     ]
    }
   ],
   "source": [
    "random = np.random.randint(len(text))\n",
    "sample_X = text[random:random+BLOCK_SIZE+1] # character block width\n",
    "print(sample_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab6d5fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56,  ..., 45,  8,  0]) 1115394\n"
     ]
    }
   ],
   "source": [
    "text_tokenized = encode_chars(text)\n",
    "text_tokenized = torch.tensor(text_tokenized, dtype=torch.long)\n",
    "print(text_tokenized, len(text_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b8b208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*(len(text_tokenized)))\n",
    "train_data = text_tokenized[:n]\n",
    "val_data = text_tokenized[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c02fd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : tensor([18], dtype=torch.int32) Output : 47\n",
      "Input : tensor([18, 47], dtype=torch.int32) Output : 56\n",
      "Input : tensor([18, 47, 56], dtype=torch.int32) Output : 57\n",
      "Input : tensor([18, 47, 56, 57], dtype=torch.int32) Output : 58\n",
      "Input : tensor([18, 47, 56, 57, 58], dtype=torch.int32) Output : 1\n",
      "Input : tensor([18, 47, 56, 57, 58,  1], dtype=torch.int32) Output : 15\n",
      "Input : tensor([18, 47, 56, 57, 58,  1, 15], dtype=torch.int32) Output : 47\n",
      "Input : tensor([18, 47, 56, 57, 58,  1, 15, 47], dtype=torch.int32) Output : 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:BLOCK_SIZE]\n",
    "y = train_data[1:BLOCK_SIZE+1]\n",
    "for t in range(BLOCK_SIZE//4):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Input : {context} Output : {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84d70707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n",
    "    x = torch.stack([text_tokenized[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([text_tokenized[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de3ce381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train') # x_batch, y_batch\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0630c8a9",
   "metadata": {},
   "source": [
    "### **Bigram Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d057b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx) \n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1) # (B, C) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "71221837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dVpYUavZh?rJUw-h?W?C'wzs&ThzPYnkmwtRfVxmGvvt;? nhzq PpmglB!zCekrA sSyb3CXKMlD;ClNVfMTzAg-nkOwh?ltpXp\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "\n",
    "print(\"\".join(map(str, decode_chars(m.generate(idx=torch.zeros((1, 1), dtype=torch.long),\n",
    "             max_new_tokens=100)[0].tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d60af699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=3e-3)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d21c733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5362985134124756\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37fa37da",
   "metadata": {},
   "source": [
    "### **Much Better Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "17503d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KICe.\n",
      "\n",
      "Anerngrey, d. wagalorgisthe cotiomim.\n",
      "PRLAhind Ththuto'd rorelat w fy me ene to bigead wit YC\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(map(str, decode_chars(m.generate(idx=torch.zeros((1, 1), dtype=torch.long),\n",
    "             max_new_tokens=100)[0].tolist()))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2df7a535",
   "metadata": {},
   "source": [
    "## **Self-Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e7c895a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73bf7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] #(t, c)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "45fc1f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8222, -0.6336],\n",
      "        [ 0.5039, -0.6146],\n",
      "        [ 0.8172,  0.2961],\n",
      "        [-0.0206,  0.9294],\n",
      "        [-0.1935,  0.0326],\n",
      "        [ 0.2557, -0.9186],\n",
      "        [-0.9985, -0.2627],\n",
      "        [-0.3635, -0.6702]])\n",
      "tensor([[-0.8222, -0.6336],\n",
      "        [-0.1591, -0.6241],\n",
      "        [ 0.1663, -0.3174],\n",
      "        [ 0.1196, -0.0057],\n",
      "        [ 0.0570,  0.0020],\n",
      "        [ 0.0901, -0.1514],\n",
      "        [-0.0654, -0.1673],\n",
      "        [-0.1027, -0.2302]])\n"
     ]
    }
   ],
   "source": [
    "print(x[0], xbow[0], sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62b38914",
   "metadata": {},
   "source": [
    "### **Trick Matrix Multiplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "646ac124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [8., 2.]])\n",
      "tensor([[0.0000, 1.0000],\n",
      "        [1.0000, 2.0000],\n",
      "        [3.3333, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a@b # dot product\n",
    "print(a, b, c, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e3c17d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Version 2\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei/wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) --> (B, T, C) \n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e94077e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 3\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e520312",
   "metadata": {},
   "source": [
    "- q => query vector = represents what do i need\n",
    "- k => key vector = represents what do i contain\n",
    "- v => it was i will communicate to you (aggregated)\n",
    "- x => private informatiom of token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8efca49",
   "metadata": {},
   "source": [
    "### version 4 : Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "76bb2a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9970e-01,  3.7562e-01,  2.7412e-02, -2.0991e-01,  1.3768e+00,\n",
       "          1.2206e+00,  2.9849e-01, -1.0155e+00,  1.1327e+00,  2.1584e-01,\n",
       "          4.4205e-01, -1.5354e+00, -1.3581e-01, -3.4468e-01, -9.0398e-02,\n",
       "         -1.2043e+00],\n",
       "        [-4.5160e-01,  3.4452e-01, -1.5435e-02, -2.0663e-01,  1.3587e+00,\n",
       "          1.1838e+00,  2.9072e-01, -9.4888e-01,  1.0739e+00,  2.1311e-01,\n",
       "          4.3928e-01, -1.4852e+00, -1.5369e-01, -3.1936e-01, -7.7948e-02,\n",
       "         -1.1700e+00],\n",
       "        [ 3.9563e-02,  9.6913e-03, -5.1651e-01, -2.7865e-01,  9.4924e-01,\n",
       "          4.3404e-01,  2.7098e-02,  8.7683e-02,  4.6060e-02,  3.2440e-01,\n",
       "          3.5108e-01, -4.2002e-01, -3.5586e-01, -4.2187e-03,  1.7380e-01,\n",
       "         -1.5972e-01],\n",
       "        [ 5.5984e-02, -1.1672e-02, -4.6565e-01, -1.7145e-01,  1.0840e+00,\n",
       "          6.9033e-01,  1.9894e-01, -1.2481e-01,  3.4810e-01,  2.0646e-01,\n",
       "          3.8111e-01, -8.8187e-01, -3.3997e-01, -2.3965e-02,  6.5496e-02,\n",
       "         -7.2805e-01],\n",
       "        [-3.7900e-01,  1.5915e-01,  6.8070e-02, -1.7130e-01,  7.7648e-01,\n",
       "          4.6357e-01,  9.5400e-02, -1.0859e-01,  4.8457e-01,  3.8819e-01,\n",
       "          1.7496e-01, -9.3238e-01, -1.8496e-02, -1.3578e-01,  1.3204e-01,\n",
       "         -5.7743e-01],\n",
       "        [ 1.4536e-02,  2.1094e-01, -1.0638e-01, -1.6393e-01,  6.2466e-01,\n",
       "          4.2871e-01,  1.6365e-01, -2.4657e-01,  5.1345e-01, -1.3415e-01,\n",
       "         -3.6613e-01, -4.7960e-01, -2.8079e-01, -2.6454e-01,  2.5651e-01,\n",
       "         -2.8705e-01],\n",
       "        [ 1.5891e-03,  3.4574e-02, -1.1878e-01, -1.6338e-01,  7.4098e-01,\n",
       "          3.8023e-01, -2.4511e-01,  2.8802e-02,  4.3214e-01,  3.7925e-01,\n",
       "          1.4264e-01, -3.3251e-01, -1.1174e-02, -5.7080e-02,  3.9247e-01,\n",
       "         -7.7428e-02],\n",
       "        [ 1.6077e-01, -2.0679e-01, -1.4149e-01,  6.9762e-02,  4.3705e-01,\n",
       "          1.2297e-01,  1.0537e-01,  2.0491e-02,  1.2984e-01,  6.5787e-02,\n",
       "          1.4353e-03, -1.7882e-01, -2.1730e-01,  1.4519e-01, -3.5972e-02,\n",
       "         -1.4232e-01]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,32\n",
    "HEAD_SIZE = 16\n",
    "x = torch.randn(B,T, C)\n",
    "\n",
    "query = torch.nn.Linear(C, HEAD_SIZE, bias=False)\n",
    "key = torch.nn.Linear(C, HEAD_SIZE, bias=False)\n",
    "value = torch.nn.Linear(C, HEAD_SIZE, bias=False)\n",
    "\n",
    "k = key(x)  # (B, T, C) @ (B, C, HEAD_SIZE) -> (B, T, HEAD_SIZE)\n",
    "q = query(x) # (B, T, C) @ (B, C, HEAD_SIZE) -> (B, T, HEAD_SIZE)\n",
    "v = value(x) # (B, T, C) @ (B, C, HEAD_SIZE) -> (B, T, HEAD_SIZE)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, HEAD_SIZE) @ (B, HEAD_SIZE, T) -> (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T)) # Positional Encoding\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ v\n",
    "out[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdda433b",
   "metadata": {},
   "source": [
    "### Scaled attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "16a73b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9936)\n"
     ]
    }
   ],
   "source": [
    "k=torch.randn(B, T, HEAD_SIZE)\n",
    "q=torch.randn(B, T, HEAD_SIZE)\n",
    "wei = q @ k.transpose(-2, -1) * HEAD_SIZE**-0.5\n",
    "\n",
    "print(wei.var())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab8432f1",
   "metadata": {},
   "source": [
    "## **HEAD**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "280c39a9",
   "metadata": {},
   "source": [
    "### **Self Attention Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "efc9b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(pl.LightningModule):\n",
    "    def __init__(self, n_embed, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        \n",
    "        # Saved with paramters but not updating\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-ing'))\n",
    "        wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea6a80f8",
   "metadata": {},
   "source": [
    "### **Multi Head Attention Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7b04d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(pl.LightningModule):\n",
    "    def __init__(self, num_heads, n_embed, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([Head(n_embed, head_size, block_size) for _ in range(num_heads)])\n",
    "        self.proj = torch.nn.Linear(n_embed, n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=1)\n",
    "        return self.proj(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b96b4025",
   "metadata": {},
   "source": [
    "### **Feed Forward Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "5f15c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(pl.LightningModule):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embed, 4*n_embed),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4*n_embed, n_embed),\n",
    "            torch.nn.Dropout(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f4e892d",
   "metadata": {},
   "source": [
    "### **Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8e5b6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(pl.LightningModule):\n",
    "    def __init__(self, num_heads, n_embed, block_size, vocab_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // num_heads\n",
    "        self.sa_head = MultiHeadAttention(num_heads, n_embed, head_size//4, block_size) # Self Attention Head\n",
    "        self.ffwd = FeedForwardLayer(n_embed)\n",
    "        self.ln1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.ln2 = torch.nn.LayerNorm(n_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a576d4",
   "metadata": {},
   "source": [
    "## **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "b884fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size:int, n_embed:int, block_size:int, num_heads:int):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding_table = torch.nn.Embedding(block_size, n_embed)\n",
    "        self.transfomer_blocks = torch.nn.Sequential(\n",
    "            TransformerBlock(num_heads, n_embed, block_size, vocab_size),\n",
    "            TransformerBlock(num_heads, n_embed, block_size, vocab_size),\n",
    "            TransformerBlock(num_heads, n_embed, block_size, vocab_size),\n",
    "            TransformerBlock(num_heads, n_embed, block_size, vocab_size),\n",
    "            torch.nn.LayerNorm(n_embed),\n",
    "        )\n",
    "        self.block_size = block_size\n",
    "        self.lm_head = torch.nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embedding = self.token_embedding_table(idx) # (B, T, C)\n",
    "        logits = self.lm_head(token_embedding) # (B, T, vocab_size)\n",
    "        positional_embedding = self.positional_embedding_table(torch.arange(T))\n",
    "        x = token_embedding + positional_embedding\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx= idx[:, -self.block_size: ]\n",
    "            logits, loss = self(idx) \n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1) # (B, C) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "89bfca34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 32)\n",
       "  (positional_embedding_table): Embedding(32, 32)\n",
       "  (transfomer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (sa_head): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=2, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=2, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=2, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ffwd): FeedForwardLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (sa_head): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=2, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=2, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=2, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ffwd): FeedForwardLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (sa_head): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=2, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=2, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=2, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ffwd): FeedForwardLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (sa_head): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=2, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=2, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=2, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ffwd): FeedForwardLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_HEADS=4\n",
    "m = LanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embed=EMBEDDING_SIZE,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    num_heads=NUM_HEADS\n",
    ")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7787c156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dbbcd6b",
   "metadata": {},
   "source": [
    "### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "3523bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4276676177978516\n"
     ]
    }
   ],
   "source": [
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f7a55f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ribu wongrin pownce ld were,\n",
      "An s\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"\".join(map(str, decode_chars(m.generate(idx=torch.zeros((1, 1), dtype=torch.long),\n",
    "                 max_new_tokens=30)[0].tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e041495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
