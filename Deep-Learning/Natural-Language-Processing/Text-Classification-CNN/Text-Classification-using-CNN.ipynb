{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Classification Using CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Author: Navin Kumar M 20BAI1094\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.10\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "torch : 2.1.0a0+fe05266\n",
      "pandas: 1.5.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Navin Kumar M 20BAI1094' -v -p torch,pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext as tt\n",
    "\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import re\n",
    "\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(seed=SEED)\n",
    "torch.cuda.manual_seed_all(seed=SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int = 400000\n",
    "    embedding_dim: int = 100\n",
    "    max_seq_len: int = 30\n",
    "    pretrain_emb: bool = False\n",
    "    epochs: int = 10\n",
    "    batch_size: int = 32\n",
    "    lr: float = 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dataset_path = 'data/rt-polarity.neg.txt'\n",
    "pos_dataset_path = 'data/rt-polarity.pos.txt'\n",
    "\n",
    "# read and return the dataset as pandas dataframe\n",
    "def dataset_return(path: str) -> pd.DataFrame:\n",
    "    # Check the encoding of the file\n",
    "    with open(neg_dataset_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(3500))\n",
    "    \n",
    "    return pd.read_csv(neg_dataset_path, encoding=result['encoding'], delimiter='\\0', header=None, names=['text']) \n",
    "\n",
    "neg_dataset = dataset_return(path=neg_dataset_path)\n",
    "pos_dataset = dataset_return(path=pos_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a momentary escape from the summer heat and th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the picture emerges as a surprisingly anemic d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>. . . pays tribute to heroes the way julia ro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the sweetest thing leaves an awful sour taste .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it doesn't do the original any particular dish...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  a momentary escape from the summer heat and th...      1\n",
       "1  the picture emerges as a surprisingly anemic d...      1\n",
       "2   . . . pays tribute to heroes the way julia ro...      0\n",
       "3   the sweetest thing leaves an awful sour taste .       1\n",
       "4  it doesn't do the original any particular dish...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the two datasets\n",
    "neg_dataset['label'] = 0\n",
    "pos_dataset['label'] = 1\n",
    "\n",
    "dataset = pd.concat(\n",
    "    [neg_dataset, pos_dataset], ignore_index=True\n",
    "    ).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a momentary escape from the summer heat and th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the picture emerges as a surprisingly anemic d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pays tribute to heroes the way julia roberts h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the sweetest thing leaves an awful sour taste</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it does n't do the original any particular dis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  a momentary escape from the summer heat and th...      1\n",
       "1  the picture emerges as a surprisingly anemic d...      1\n",
       "2  pays tribute to heroes the way julia roberts h...      0\n",
       "3      the sweetest thing leaves an awful sour taste      1\n",
       "4  it does n't do the original any particular dis...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean each string :\n",
    "def clean_str(string: str):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(lambda x: clean_str(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
       "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
       "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
       "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
       "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
       "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
       "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
       "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
       "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
       "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
       "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
       "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
       "        -0.5203, -0.1459,  0.8278,  0.2706])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_emb = GloVe(name='6B', dim=100, cache='./.vector_cache')\n",
    "token_emb.vectors[token_emb.stoi[\"the\"]] # get the vector for the word 'the'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PyTorch Dataset** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TextClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pretrained_emb = Config.pretrain_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.dataset['text'][idx], self.dataset['label'][idx] \n",
    "        sentence = []\n",
    "        if self.pretrained_emb:\n",
    "            for word in text.split(' '):\n",
    "                if word in self.tokenizer.stoi:\n",
    "                    sentence.append(\n",
    "                        self.tokenizer.vectors[self.tokenizer.stoi[word]]\n",
    "                    )\n",
    "                else:\n",
    "                    # Handle OOV word \n",
    "                    sentence.append(torch.zeros(self.tokenizer.dim))\n",
    "            sentence = torch.stack(sentence)\n",
    "\n",
    "            # Pad the sentence if it is shorter than max_seq_len else truncate it\n",
    "            if sentence.shape[0] < Config.max_seq_len:\n",
    "                # pad with zeros\n",
    "                pad_tensor = torch.zeros(Config.max_seq_len - sentence.shape[0], self.tokenizer.dim)\n",
    "                sentence = torch.cat((sentence, pad_tensor), dim=0)\n",
    "            else:\n",
    "                sentence = sentence[:Config.max_seq_len]\n",
    "\n",
    "        else:\n",
    "            for word in text.split(' '):\n",
    "                if word in self.tokenizer.stoi:\n",
    "                    sentence.append(self.tokenizer.stoi[word])\n",
    "                else:\n",
    "                    # Handle OOV word\n",
    "                    sentence.append(Config.vocab_size)\n",
    "            sentence = torch.tensor(sentence, dtype=torch.long)\n",
    "        \n",
    "            # Pad the sentence if it is shorter than max_seq_len else truncate it\n",
    "            if sentence.shape[0] < Config.max_seq_len:\n",
    "                # pad with pading token id => vocab_size-1\n",
    "                pad_tensor = torch.ones(Config.max_seq_len - sentence.shape[0], dtype=torch.long) * (Config.vocab_size)\n",
    "                sentence = torch.cat((sentence, pad_tensor), dim=0)\n",
    "            else:\n",
    "                sentence = sentence[:Config.max_seq_len]\n",
    "\n",
    "\n",
    "\n",
    "        return sentence, torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PyTorch Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pretrain_emb = Config.pretrain_emb\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            Config.vocab_size+1, Config.embedding_dim)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(3, Config.embedding_dim))\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(4, Config.embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(5, Config.embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(300, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.pretrain_emb:\n",
    "            x = self.word_embeddings(x)\n",
    "    \n",
    "        x = x.unsqueeze(1)\n",
    "        x1 = torch.relu(self.conv1(x)).squeeze(3)\n",
    "        x1 = torch.max_pool1d(x1, x1.shape[2]).squeeze(2)\n",
    "        x2 = torch.relu(self.conv2(x)).squeeze(3)\n",
    "        x2 = torch.max_pool1d(x2, x2.shape[2]).squeeze(2)\n",
    "        x3 = torch.relu(self.conv3(x)).squeeze(3)\n",
    "        x3 = torch.max_pool1d(x3, x3.shape[2]).squeeze(2)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:30<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 0.6730575561523438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 275.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Valid Loss: 0.7269708514213562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:29<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 0.7026622295379639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 320.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Valid Loss: 0.6884169578552246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:30<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 0.6836161613464355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 236.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Valid Loss: 0.7523573637008667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:30<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 0.6575700640678406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 169.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Valid Loss: 0.7961407899856567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:32<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 0.6806485056877136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 239.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Valid Loss: 0.7665050029754639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:31<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train Loss: 0.7241405844688416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 195.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Valid Loss: 0.7670677900314331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:31<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train Loss: 0.664065957069397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 210.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Valid Loss: 0.7629090547561646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:31<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train Loss: 0.6928980350494385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 268.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Valid Loss: 0.8330962061882019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 252/267 [00:30<00:01,  8.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_156971/1377643140.py\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "data = TextClassificationDataset(dataset=dataset, tokenizer=token_emb)\n",
    "model = TextCNN().to(device) \n",
    "\n",
    "train_size = int(0.8 * len(data))\n",
    "train_data, valid_data = torch.utils.data.random_split(data, [train_size, len(data) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=Config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=Config.batch_size, shuffle=True)\n",
    "optimizer = Adam(model.parameters(), lr=Config.lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and validate the model\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch} | Train Loss: {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(valid_loader):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)  \n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        print(f'Epoch: {epoch} | Valid Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 40.12M trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# get total number of model parameters in the model in Millions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000\n",
    "\n",
    "print(f'The model has {count_parameters(model):.2f}M trainable parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
